Metadata-Version: 2.1
Name: sarathi
Version: 0.1.0
Summary: A high-throughput and low-latency LLM inference system
Author: Sarathi Team
License: Apache 2.0
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: ninja
Requires-Dist: psutil
Requires-Dist: ray>=2.5.1
Requires-Dist: pandas
Requires-Dist: pyarrow
Requires-Dist: sentencepiece
Requires-Dist: numpy
Requires-Dist: torch==2.4.0
Requires-Dist: transformers>=4.37.0
Requires-Dist: matplotlib
Requires-Dist: plotly_express
Requires-Dist: seaborn
Requires-Dist: wandb
Requires-Dist: kaleido
Requires-Dist: ddsketch
Requires-Dist: jupyterlab
Requires-Dist: flashinfer>=0.0.5
Requires-Dist: pillow
Requires-Dist: tiktoken
Requires-Dist: grpcio
Requires-Dist: uvicorn
Requires-Dist: fastapi
Requires-Dist: openai

# Sarathi-Serve

This is the official OSDI'24 artifact submission for paper #444, "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve‚Äù.

## Setup

### Setup CUDA

Sarathi-Serve has been tested with CUDA 12.1 on A100 and A40 GPUs.

### Clone repository

```sh
git clone https://msri@dev.azure.com/msri/AI-Infrastructure/_git/llm-batching
```

### Create mamba environment

Setup mamba if you don't already have it,

```sh
wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh
bash Mambaforge-Linux-x86_64.sh # follow the instructions from there
```

Create a Python 3.10 environment,

```sh
mamba create -p ./env python=3.10  
```

### Install Sarathi-Serve

```sh
pip install -e . --extra-index-url https://flashinfer.ai/whl/cu121/torch2.3/
```

## Reproducing Results

Refer to readmes in individual folders corresponding to each figure in `osdi-experiments`.

## Citation

If you use our work, please consider citing our paper:

```
@article{agrawal2024taming,
  title={Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={Proceedings of 18th USENIX Symposium on Operating Systems Design and Implementation, 2024, Santa Clara},
  year={2024}
}
```

## Acknowledgment

This repository originally started as a fork of the [vLLM project](https://vllm-project.github.io/). Sarathi-Serve is a research prototype and does not have complete feature parity with open-source vLLM. We have only retained the most critical features and adopted the codebase for faster research iterations.
